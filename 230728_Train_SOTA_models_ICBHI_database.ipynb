{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 압축파일 풀기\n",
    "# !unzip data/larger_dataset_with_ICBHI.zip -d data/ICBHI_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "from time import time\n",
    "\n",
    "import natsort\n",
    "from itertools import count\n",
    "# import torch\n",
    "# from pydub import AudioSegment, silence\n",
    "\n",
    "# Math\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# # Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "import librosa.display\n",
    "from IPython.display import Audio\n",
    "from IPython.display import HTML\n",
    "\n",
    "import itertools\n",
    "import noisereduce as nr\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "import sklearn\n",
    "\n",
    "# keras module\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv1D, Conv2D, MaxPooling1D, Embedding, LSTM, Bidirectional, SimpleRNN, GRU\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "\n",
    "# Hyperparameters\n",
    "freq_lim = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "# import mglearn\n",
    "from os.path import join\n",
    "\n",
    "from sklearn.model_selection import train_test_split, train_test_split, cross_val_score, KFold\n",
    "from sklearn.preprocessing import MinMaxScaler, label_binarize, OneHotEncoder, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer,accuracy_score,f1_score,roc_auc_score,roc_curve,confusion_matrix,auc\n",
    "from sklearn.metrics import RocCurveDisplay,ConfusionMatrixDisplay\n",
    "# import umap\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 2518496707027860875,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 10269412333464437645\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 9569401596706835270\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:XLA_GPU:1\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 11100839013144696641\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:XLA_GPU:2\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 7721027403578253651\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:XLA_GPU:3\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 18386846825326975089\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 295960576\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 12137356711183827037\n",
       " physical_device_desc: \"device: 0, name: NVIDIA TITAN RTX, pci bus id: 0000:3b:00.0, compute capability: 7.5\",\n",
       " name: \"/device:GPU:1\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 23794905908\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 8911658020783086509\n",
       " physical_device_desc: \"device: 1, name: NVIDIA TITAN RTX, pci bus id: 0000:5e:00.0, compute capability: 7.5\",\n",
       " name: \"/device:GPU:2\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 310312960\n",
       " locality {\n",
       "   bus_id: 2\n",
       "   numa_node: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 17307233217895474769\n",
       " physical_device_desc: \"device: 2, name: NVIDIA TITAN RTX, pci bus id: 0000:af:00.0, compute capability: 7.5\",\n",
       " name: \"/device:GPU:3\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 23794905908\n",
       " locality {\n",
       "   bus_id: 2\n",
       "   numa_node: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 10174593199328046521\n",
       " physical_device_desc: \"device: 3, name: NVIDIA TITAN RTX, pci bus id: 0000:d8:00.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funcition definition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_dataset_sklearn(file_path, ordered_file_list, n_mels, frame_sec, step_sec, label_ratio):\n",
    "    \n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    pitch_var = [-3.5, -2, 2, 3.5]\n",
    "    augmented_number = len(pitch_var)\n",
    "\n",
    "    for index, file in enumerate(ordered_file_list):\n",
    "        \n",
    "        if file[-3:] == 'wav':\n",
    "\n",
    "            # data load \n",
    "            sample_rate = librosa.get_samplerate(file_path + '/' + file)\n",
    "            # calculated values\n",
    "            frame_length = int(frame_sec * sample_rate)\n",
    "            frame_step = int(step_sec * sample_rate)\n",
    "            data, data_sr = librosa.load(file_path + '/' + file, sr = sample_rate)\n",
    "            \n",
    "            for pitch in pitch_var:\n",
    "                \n",
    "                ps_data = librosa.effects.pitch_shift(data, sr = sample_rate, n_steps = pitch)\n",
    "\n",
    "                # mel spectrogram extraction\n",
    "                melS = librosa.feature.melspectrogram(y = ps_data, sr=sample_rate, n_mels=n_mels,\n",
    "                                                      hop_length=frame_step, win_length=frame_length)\n",
    "                melS = librosa.power_to_db(melS, ref=np.max)\n",
    "\n",
    "                data_list.append(melS.T)\n",
    "                \n",
    "            # normal and wheeze split\n",
    "            normal_event = []\n",
    "            wheeze_event = []\n",
    "                \n",
    "            label = pd.read_csv(file_path + '/' + file[:-3] + 'txt', sep='\\t', header=None)\n",
    "            \n",
    "            sec_to_index_start = label[0]*sample_rate\n",
    "            sec_to_index_end = label[1]*sample_rate\n",
    "            sec_to_index_start = sec_to_index_start.to_numpy().round()\n",
    "            sec_to_index_end = sec_to_index_end.to_numpy().round()\n",
    "\n",
    "            for i, val in enumerate(label[2].to_numpy()):\n",
    "                if val == 'normal':\n",
    "                    normal_event.append([int(sec_to_index_start[i]//(step_sec*sample_rate)),\n",
    "                                         int(sec_to_index_end[i]//(step_sec*sample_rate))])\n",
    "\n",
    "                elif val == 'wheeze':\n",
    "                    wheeze_event.append([int(sec_to_index_start[i]//(step_sec*sample_rate)),\n",
    "                                         int(sec_to_index_end[i]//(step_sec*sample_rate))])\n",
    "\n",
    "            normal_event = np.array(normal_event)\n",
    "            wheeze_event = np.array(wheeze_event)\n",
    "            \n",
    "            y = np.zeros(len(data_list[-1]))\n",
    " \n",
    "            # label ratio decision\n",
    "            for j in normal_event:\n",
    "                start = j[0]\n",
    "                end = j[1]\n",
    "                delta = int((end-start)*label_ratio)\n",
    "                y[end-delta:end] = 1\n",
    "\n",
    "            for k in wheeze_event:\n",
    "                start = k[0]\n",
    "                end = k[1]\n",
    "                delta = int((end-start)*label_ratio)\n",
    "                y[end-delta:end] = 2\n",
    "                \n",
    "            for number in range(augmented_number):\n",
    "                label_list.append(y)\n",
    "                \n",
    "    X = np.concatenate(data_list, axis = 0)\n",
    "#     X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    y = np.concatenate(label_list, axis = 0)\n",
    "#     y = utils.to_categorical(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_validation_dataset_sklearn(file_path, ordered_file_list, n_mels, frame_sec, step_sec, label_ratio):\n",
    "\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for index, file in enumerate(ordered_file_list):\n",
    "        if file[-3:] == 'wav':\n",
    "            \n",
    "            # data load \n",
    "            sample_rate = librosa.get_samplerate(file_path + '/' + file)\n",
    "            data, data_sr = librosa.load(file_path + '/' + file, sr = sample_rate)\n",
    "\n",
    "            # framing condition\n",
    "            frame_length = int(frame_sec * sample_rate)\n",
    "            frame_step = int(step_sec * sample_rate)\n",
    "\n",
    "            # mel spectrogram extraction\n",
    "            melS = librosa.feature.melspectrogram(y = data, sr=sample_rate, n_mels=n_mels,\n",
    "                                                  hop_length=frame_step, win_length=frame_length)\n",
    "            melS = librosa.power_to_db(melS, ref=np.max)\n",
    "#             melS = librosa.power_to_db(melS, ref=np.median)\n",
    "\n",
    "            data_list.append(melS.T)\n",
    "\n",
    "    # X must be extracted first\n",
    "\n",
    "    for index, file in enumerate(ordered_file_list):\n",
    "\n",
    "        if file[-3:] == 'txt':\n",
    "\n",
    "            # normal and wheeze split\n",
    "            normal_event = []\n",
    "            wheeze_event = []\n",
    "\n",
    "            label = pd.read_csv(file_path + '/' + file, sep='\\t', header=None)\n",
    "\n",
    "            sec_to_index_start = label[0]*sample_rate\n",
    "            sec_to_index_end = label[1]*sample_rate\n",
    "            sec_to_index_start = sec_to_index_start.to_numpy().round()\n",
    "            sec_to_index_end = sec_to_index_end.to_numpy().round()\n",
    "\n",
    "            for i, val in enumerate(label[2].to_numpy()):\n",
    "                if val == 1:\n",
    "                    normal_event.append([int(sec_to_index_start[i]//(step_sec*sample_rate)), \n",
    "                                         int(sec_to_index_end[i]//(step_sec*sample_rate))])\n",
    "\n",
    "                elif val == 2:\n",
    "                    wheeze_event.append([int(sec_to_index_start[i]//(step_sec*sample_rate)), \n",
    "                                         int(sec_to_index_end[i]//(step_sec*sample_rate))])\n",
    "\n",
    "            normal_event = np.array(normal_event)\n",
    "            wheeze_event = np.array(wheeze_event)\n",
    "\n",
    "            y = np.zeros(len(data_list[int(index/2)]))\n",
    "\n",
    "            # label ratio decision\n",
    "\n",
    "            for j in normal_event:\n",
    "                start = j[0]\n",
    "                end = j[1]\n",
    "                delta = int((end-start)*label_ratio)\n",
    "                y[end-delta:end] = 1\n",
    "#                 y[end:end+delta] = 1\n",
    "\n",
    "\n",
    "            for k in wheeze_event:\n",
    "                start = k[0]\n",
    "                end = k[1]\n",
    "                delta = int((end-start)*label_ratio)\n",
    "                y[end-delta:end] = 2\n",
    "#                 y[end:end+delta] = 2\n",
    "\n",
    "\n",
    "            label_list.append(y)\n",
    "    \n",
    "    X = np.concatenate(data_list, axis = 0)\n",
    "    X = X.reshape(X.shape[0], X.shape[1])\n",
    "#     X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    y = np.concatenate(label_list, axis = 0)\n",
    "#     y = utils.to_categorical(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_train_dataset_keras(file_path, ordered_file_list, n_mels, frame_sec, step_sec, label_ratio):\n",
    "    \n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    pitch_var = [-3.5, -2, 2, 3.5]\n",
    "    augmented_number = len(pitch_var)\n",
    "\n",
    "    for index, file in enumerate(ordered_file_list):\n",
    "        \n",
    "        if file[-3:] == 'wav':\n",
    "\n",
    "            # data load \n",
    "            sample_rate = librosa.get_samplerate(file_path + '/' + file)\n",
    "            # calculated values\n",
    "            frame_length = int(frame_sec * sample_rate)\n",
    "            frame_step = int(step_sec * sample_rate)\n",
    "            data, data_sr = librosa.load(file_path + '/' + file, sr = sample_rate)\n",
    "            \n",
    "            for pitch in pitch_var:\n",
    "                \n",
    "                ps_data = librosa.effects.pitch_shift(data, sr = sample_rate, n_steps = pitch)\n",
    "\n",
    "                # mel spectrogram extraction\n",
    "                melS = librosa.feature.melspectrogram(y = ps_data, sr=sample_rate, n_mels=n_mels,\n",
    "                                                      hop_length=frame_step, win_length=frame_length)\n",
    "                melS = librosa.power_to_db(melS, ref=np.max)\n",
    "\n",
    "                data_list.append(melS.T)\n",
    "                \n",
    "            # normal and wheeze split\n",
    "            normal_event = []\n",
    "            wheeze_event = []\n",
    "                \n",
    "            label = pd.read_csv(file_path + '/' + file[:-3] + 'txt', sep='\\t', header=None)\n",
    "            \n",
    "            sec_to_index_start = label[0]*sample_rate\n",
    "            sec_to_index_end = label[1]*sample_rate\n",
    "            sec_to_index_start = sec_to_index_start.to_numpy().round()\n",
    "            sec_to_index_end = sec_to_index_end.to_numpy().round()\n",
    "\n",
    "            for i, val in enumerate(label[2].to_numpy()):\n",
    "                if val == 'normal':\n",
    "                    normal_event.append([int(sec_to_index_start[i]//(step_sec*sample_rate)),\n",
    "                                         int(sec_to_index_end[i]//(step_sec*sample_rate))])\n",
    "\n",
    "                elif val == 'wheeze':\n",
    "                    wheeze_event.append([int(sec_to_index_start[i]//(step_sec*sample_rate)),\n",
    "                                         int(sec_to_index_end[i]//(step_sec*sample_rate))])\n",
    "\n",
    "            normal_event = np.array(normal_event)\n",
    "            wheeze_event = np.array(wheeze_event)\n",
    "            \n",
    "            y = np.zeros(len(data_list[-1]))\n",
    " \n",
    "            # label ratio decision\n",
    "            for j in normal_event:\n",
    "                start = j[0]\n",
    "                end = j[1]\n",
    "                delta = int((end-start)*label_ratio)\n",
    "                y[end-delta:end] = 1\n",
    "\n",
    "            for k in wheeze_event:\n",
    "                start = k[0]\n",
    "                end = k[1]\n",
    "                delta = int((end-start)*label_ratio)\n",
    "                y[end-delta:end] = 2\n",
    "                \n",
    "            for number in range(augmented_number):\n",
    "                label_list.append(y)\n",
    "                \n",
    "    X = np.concatenate(data_list, axis = 0)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    y = np.concatenate(label_list, axis = 0)\n",
    "    y = utils.to_categorical(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_validation_dataset_keras(file_path, ordered_file_list, n_mels, frame_sec, step_sec, label_ratio):\n",
    "\n",
    "\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "\n",
    "    for index, file in enumerate(ordered_file_list):\n",
    "        if file[-3:] == 'wav':\n",
    "\n",
    "            # data load \n",
    "            sample_rate = librosa.get_samplerate(file_path + '/' + file)\n",
    "            data, data_sr = librosa.load(file_path + '/' + file, sr = sample_rate)\n",
    "\n",
    "            # framing condition\n",
    "            frame_length = int(frame_sec * sample_rate)\n",
    "            frame_step = int(step_sec * sample_rate)\n",
    "\n",
    "            # mel spectrogram extraction\n",
    "            melS = librosa.feature.melspectrogram(y = data, sr=sample_rate, n_mels=n_mels,\n",
    "                                                  hop_length=frame_step, win_length=frame_length)\n",
    "            melS = librosa.power_to_db(melS, ref=np.max)\n",
    "#             melS = librosa.power_to_db(melS, ref=np.median)\n",
    "\n",
    "            data_list.append(melS.T)\n",
    "\n",
    "    # X must be extracted first\n",
    "\n",
    "    for index, file in enumerate(ordered_file_list):\n",
    "\n",
    "        if file[-3:] == 'txt':\n",
    "\n",
    "            # normal and wheeze split\n",
    "            normal_event = []\n",
    "            wheeze_event = []\n",
    "\n",
    "            label = pd.read_csv(file_path + '/' + file, sep='\\t', header=None)\n",
    "\n",
    "            sec_to_index_start = label[0]*sample_rate\n",
    "            sec_to_index_end = label[1]*sample_rate\n",
    "            sec_to_index_start = sec_to_index_start.to_numpy().round()\n",
    "            sec_to_index_end = sec_to_index_end.to_numpy().round()\n",
    "\n",
    "            for i, val in enumerate(label[2].to_numpy()):\n",
    "                if val == 1:\n",
    "                    normal_event.append([int(sec_to_index_start[i]//(step_sec*sample_rate)), \n",
    "                                         int(sec_to_index_end[i]//(step_sec*sample_rate))])\n",
    "\n",
    "                elif val == 2:\n",
    "                    wheeze_event.append([int(sec_to_index_start[i]//(step_sec*sample_rate)), \n",
    "                                         int(sec_to_index_end[i]//(step_sec*sample_rate))])\n",
    "\n",
    "            normal_event = np.array(normal_event)\n",
    "            wheeze_event = np.array(wheeze_event)\n",
    "\n",
    "            y = np.zeros(len(data_list[int(index/2)]))\n",
    "\n",
    "            # label ratio decision\n",
    "\n",
    "            for j in normal_event:\n",
    "                start = j[0]\n",
    "                end = j[1]\n",
    "                delta = int((end-start)*label_ratio)\n",
    "                y[end-delta:end] = 1\n",
    "#                 y[end:end+delta] = 1\n",
    "\n",
    "\n",
    "            for k in wheeze_event:\n",
    "                start = k[0]\n",
    "                end = k[1]\n",
    "                delta = int((end-start)*label_ratio)\n",
    "                y[end-delta:end] = 2\n",
    "#                 y[end:end+delta] = 2\n",
    "\n",
    "\n",
    "            label_list.append(y)\n",
    "    \n",
    "    X = np.concatenate(data_list, axis = 0)\n",
    "    X = X.reshape(X.shape[0], X.shape[1])\n",
    "    X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "    y = np.concatenate(label_list, axis = 0)\n",
    "    y = utils.to_categorical(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def make_test_dataset_keras(test_signal, test_label, n_mels, frame_sec, step_sec):\n",
    "    \n",
    "    # test signal must be noise filtered\n",
    "\n",
    "    # framing condition\n",
    "    frame_length = int(frame_sec * sample_rate)\n",
    "    frame_step = int(step_sec * sample_rate)\n",
    "\n",
    "    # mel spectrogram extraction\n",
    "    melS = librosa.feature.melspectrogram(y = test_signal, sr=sample_rate, \n",
    "                                          n_mels=n_mels, hop_length=frame_step, win_length=frame_length)\n",
    "    melS = librosa.power_to_db(melS, ref=np.max)\n",
    "#     melS = librosa.power_to_db(melS, ref=np.median)\n",
    "\n",
    "    X_test = melS.T\n",
    "    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1],1)\n",
    "    \n",
    "    sec_to_index = test_label[0]*sample_rate\n",
    "    event_index = sec_to_index//(step_sec*sample_rate)\n",
    "\n",
    "    y_test = np.zeros(len(X_test))\n",
    "\n",
    "    for i in event_index:\n",
    "        y_test[int(i)] = 2\n",
    "        \n",
    "    return X_test, y_test\n",
    "\n",
    "\n",
    "def wheeze_per_resp_acc(y_pred, y_test):\n",
    "    \n",
    "    peaks, _ = find_peaks(y_pred)\n",
    "\n",
    "    after_peaks = peaks[1:]\n",
    "    before_peaks = peaks[:-1]\n",
    "\n",
    "    term = after_peaks - before_peaks\n",
    "\n",
    "    start = 0\n",
    "    pred_resp_count = 0\n",
    "    pred_normal_count = 0\n",
    "    pred_wheeze_count = 0\n",
    "\n",
    "    for index, val in enumerate(term):\n",
    "        if val >= 100:\n",
    "            pred_resp_count += 1\n",
    "            thres = y_pred[peaks[start:index+1]].mean()\n",
    "            if thres > 1.0:\n",
    "                pred_wheeze_count += 1\n",
    "            elif thres == 1.0 : \n",
    "                pred_normal_count += 1\n",
    "            start = index+1\n",
    "    \n",
    "    print('prediction')\n",
    "    print(pred_wheeze_count, '/', pred_resp_count)\n",
    "    \n",
    "    peaks, _ = find_peaks(y_test)\n",
    "\n",
    "    after_peaks = peaks[1:]\n",
    "    before_peaks = peaks[:-1]\n",
    "\n",
    "    term = after_peaks - before_peaks\n",
    "\n",
    "    start = 0\n",
    "    test_resp_count = 0\n",
    "    test_normal_count = 0\n",
    "    test_wheeze_count = 0\n",
    "\n",
    "    for index, val in enumerate(term):\n",
    "        if val >= 100:\n",
    "            test_resp_count += 1\n",
    "            thres = y_test[peaks[start:index+1]].mean()\n",
    "            if thres > 1.0:\n",
    "                test_wheeze_count += 1\n",
    "            elif thres == 1.0 : \n",
    "                test_normal_count += 1\n",
    "            start = index+1\n",
    "             \n",
    "    print('test')\n",
    "    print(test_wheeze_count, '/', test_resp_count)\n",
    "    \n",
    "    return pred_wheeze_count, pred_resp_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n",
    "                       model, param_grid, cv=5, scoring_fit='roc_auc',\n",
    "                       do_probabilities = False):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid, \n",
    "#         cv=cv, \n",
    "        n_jobs=1, \n",
    "#         scoring=scoring_fit,\n",
    "#         verbose=2\n",
    "    )\n",
    "    \n",
    "    gs.fit(X_train_data, y_train_data)\n",
    "    \n",
    "    if do_probabilities:\n",
    "        pred = gs.predict_proba(X_test_data)\n",
    "    else:\n",
    "        pred = gs.predict(X_test_data)\n",
    "    \n",
    "    return gs, pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fixed parameter results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['103_2b2_Ar_mc_LittC2SE.txt',\n",
       " '103_2b2_Ar_mc_LittC2SE.wav',\n",
       " '106_2b1_Pr_mc_LittC2SE.txt',\n",
       " '106_2b1_Pr_mc_LittC2SE.wav',\n",
       " '121_1b1_Tc_sc_Meditron.txt',\n",
       " '121_1b1_Tc_sc_Meditron.wav',\n",
       " '133_2p4_Al_mc_AKGC417L.txt',\n",
       " '133_2p4_Al_mc_AKGC417L.wav',\n",
       " '134_2b3_Ar_mc_LittC2SE.txt',\n",
       " '134_2b3_Ar_mc_LittC2SE.wav',\n",
       " '141_1b2_Lr_mc_LittC2SE.txt',\n",
       " '141_1b2_Lr_mc_LittC2SE.wav',\n",
       " '143_1b1_Al_sc_Meditron.txt',\n",
       " '143_1b1_Al_sc_Meditron.wav',\n",
       " '144_1b1_Al_sc_Meditron.txt',\n",
       " '144_1b1_Al_sc_Meditron.wav',\n",
       " '151_2p2_Pl_mc_AKGC417L.txt',\n",
       " '151_2p2_Pl_mc_AKGC417L.wav',\n",
       " '152_1b1_Al_sc_Meditron.txt',\n",
       " '152_1b1_Al_sc_Meditron.wav',\n",
       " '153_1b1_Al_sc_Meditron.txt',\n",
       " '153_1b1_Al_sc_Meditron.wav',\n",
       " '159_1b1_Pr_sc_Meditron.txt',\n",
       " '159_1b1_Pr_sc_Meditron.wav',\n",
       " '160_1b2_Ar_mc_AKGC417L.txt',\n",
       " '160_1b2_Ar_mc_AKGC417L.wav',\n",
       " '171_1b1_Al_sc_Meditron.txt',\n",
       " '171_1b1_Al_sc_Meditron.wav',\n",
       " '178_1b2_Tc_mc_AKGC417L.txt',\n",
       " '178_1b2_Tc_mc_AKGC417L.wav',\n",
       " '183_1b1_Tc_sc_Meditron.txt',\n",
       " '183_1b1_Tc_sc_Meditron.wav',\n",
       " '184_1b1_Ar_sc_Meditron.txt',\n",
       " '184_1b1_Ar_sc_Meditron.wav',\n",
       " '192_2b1_Ar_mc_LittC2SE.txt',\n",
       " '192_2b1_Ar_mc_LittC2SE.wav',\n",
       " '193_1b2_Al_mc_AKGC417L.txt',\n",
       " '193_1b2_Al_mc_AKGC417L.wav',\n",
       " '193_1b2_Ar_mc_AKGC417L.txt',\n",
       " '193_1b2_Ar_mc_AKGC417L.wav',\n",
       " '193_1b2_Pl_mc_AKGC417L.txt',\n",
       " '193_1b2_Pl_mc_AKGC417L.wav',\n",
       " '193_1b2_Pr_mc_AKGC417L.txt',\n",
       " '193_1b2_Pr_mc_AKGC417L.wav',\n",
       " '193_1b2_Tc_mc_AKGC417L.txt',\n",
       " '193_1b2_Tc_mc_AKGC417L.wav',\n",
       " '193_1b4_Lr_mc_AKGC417L.txt',\n",
       " '193_1b4_Lr_mc_AKGC417L.wav',\n",
       " '194_1b1_Lr_sc_Meditron.txt',\n",
       " '194_1b1_Lr_sc_Meditron.wav',\n",
       " '194_1b1_Pr_sc_Meditron.txt',\n",
       " '194_1b1_Pr_sc_Meditron.wav',\n",
       " '198_1b5_Ar_mc_AKGC417L.txt',\n",
       " '198_1b5_Ar_mc_AKGC417L.wav',\n",
       " '198_1b5_Pr_mc_AKGC417L.txt',\n",
       " '198_1b5_Pr_mc_AKGC417L.wav',\n",
       " '202_1b1_Ar_sc_Meditron.txt',\n",
       " '202_1b1_Ar_sc_Meditron.wav',\n",
       " '203_1p2_Tc_mc_AKGC417L.txt',\n",
       " '203_1p2_Tc_mc_AKGC417L.wav',\n",
       " '204_2b5_Al_mc_AKGC417L.txt',\n",
       " '204_2b5_Al_mc_AKGC417L.wav',\n",
       " '209_1b1_Tc_sc_Meditron.txt',\n",
       " '209_1b1_Tc_sc_Meditron.wav',\n",
       " '214_1b1_Ar_sc_Meditron.txt',\n",
       " '214_1b1_Ar_sc_Meditron.wav',\n",
       " '217_1b1_Tc_sc_Meditron.txt',\n",
       " '217_1b1_Tc_sc_Meditron.wav',\n",
       " '221_2b1_Pl_mc_LittC2SE.txt',\n",
       " '221_2b1_Pl_mc_LittC2SE.wav',\n",
       " '221_2b2_Al_mc_LittC2SE.txt',\n",
       " '221_2b2_Al_mc_LittC2SE.wav',\n",
       " '221_2b2_Pl_mc_LittC2SE.txt',\n",
       " '221_2b2_Pl_mc_LittC2SE.wav',\n",
       " '221_2b3_Al_mc_LittC2SE.txt',\n",
       " '221_2b3_Al_mc_LittC2SE.wav',\n",
       " '224_1b2_Al_sc_Meditron.txt',\n",
       " '224_1b2_Al_sc_Meditron.wav',\n",
       " '225_1b1_Pl_sc_Meditron.txt',\n",
       " '225_1b1_Pl_sc_Meditron.wav',\n",
       " 'normal_littmann_2cycles.txt',\n",
       " 'normal_littmann_2cycles.wav',\n",
       " 'normal_sim_7cycles.txt',\n",
       " 'normal_sim_7cycles.wav',\n",
       " 'wheeze_clinical_train.txt',\n",
       " 'wheeze_clinical_train.wav',\n",
       " 'wheeze_expiratory_littmann_2cycles.txt',\n",
       " 'wheeze_expiratory_littmann_2cycles.wav',\n",
       " 'wheeze_low_pitch_littmann_2cycles.txt',\n",
       " 'wheeze_low_pitch_littmann_2cycles.wav',\n",
       " 'wheeze_monophonic_littmann_2cycles.txt',\n",
       " 'wheeze_monophonic_littmann_2cycles.wav',\n",
       " 'wheeze_sim_5cycles.txt',\n",
       " 'wheeze_sim_5cycles.wav']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path= 'data/train_with_ICBHI'\n",
    "file_list = os.listdir(file_path)\n",
    "ordered_file_list = natsort.natsorted(file_list)\n",
    "ordered_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordered_file_list = ordered_file_list[1:]\n",
    "# ordered_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['normal_littmann_1cycle.txt',\n",
       " 'normal_littmann_1cycle.wav',\n",
       " 'normal_sim_2cycles.txt',\n",
       " 'normal_sim_2cycles.wav',\n",
       " 'wheeze_clinical_validation.txt',\n",
       " 'wheeze_clinical_validation.wav',\n",
       " 'wheeze_expiratory_littmann_1cycle.txt',\n",
       " 'wheeze_expiratory_littmann_1cycle.wav',\n",
       " 'wheeze_low_pitch_littmann_1cycle.txt',\n",
       " 'wheeze_low_pitch_littmann_1cycle.wav',\n",
       " 'wheeze_monophonic_littmann_1cycle.txt',\n",
       " 'wheeze_monophonic_littmann_1cycle.wav',\n",
       " 'wheeze_sim_2cycles.txt',\n",
       " 'wheeze_sim_2cycles.wav']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_file_path= 'data/validation'\n",
    "\n",
    "val_file_list = os.listdir(val_file_path)\n",
    "val_ordered_file_list = natsort.natsorted(val_file_list)\n",
    "\n",
    "val_ordered_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_ordered_file_list = val_ordered_file_list[1:]\n",
    "# val_ordered_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mels = 128\n",
    "frame_sec = 0.025\n",
    "step_sec = 0.01\n",
    "label_ratio = 1\n",
    "\n",
    "X, y = make_train_dataset_sklearn(file_path, ordered_file_list, n_mels, frame_sec, step_sec, label_ratio)\n",
    "X_test, y_test = make_validation_dataset_sklearn(val_file_path, val_ordered_file_list, n_mels, frame_sec, step_sec, label_ratio)\n",
    "#%% Split data\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False)\n",
    "# n_timesteps, n_features, n_outputs = X.shape[1], X.shape[2], y.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(332720, 128)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with k-fold, scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test with fold: 1 : 0.7964955518153403\n",
      "test with fold: 2 : 0.8473190670834335\n",
      "test with fold: 3 : 0.7742546285164703\n",
      "test with fold: 4 : 0.8007934599663381\n",
      "test with fold: 5 : 0.7973972108679971\n",
      "test with fold: 6 : 0.789522721808127\n",
      "test with fold: 7 : 0.8045503726857418\n",
      "test with fold: 8 : 0.7824897811974032\n",
      "test with fold: 9 : 0.7970966578504448\n",
      "test with fold: 10 : 0.4298809810050493\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=False)\n",
    "\n",
    "cv_model = []\n",
    "cv_score = []\n",
    "\n",
    "fold_number = 0\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "    \n",
    "    fold_number += 1\n",
    "    \n",
    "    clf = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=0))  \n",
    "#     clf = RandomForestClassifier(random_state=0)\n",
    "    \n",
    "    train_x, val_x = X[train_index], X[val_index]\n",
    "    train_y, val_y = y[train_index], y[val_index]\n",
    "    \n",
    "    clf.fit(train_x, train_y)\n",
    "    score = clf.score(val_x, val_y)\n",
    "    \n",
    "    cv_model.append(clf)\n",
    "    cv_score.append(score)\n",
    "    \n",
    "    print('test with fold:', fold_number, ':', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.8362732919254658\n",
      "acc: 0.8362732919254658\n",
      "f1: 0.8414140628249657\n",
      "roc_auc: 0.9555164463021505\n",
      "[[0.85278569 0.00244002 0.1447743 ]\n",
      " [0.08094435 0.82293423 0.09612142]\n",
      " [0.06988695 0.1274409  0.80267215]]\n"
     ]
    }
   ],
   "source": [
    "best_index = np.argmax(cv_score)\n",
    "print(best_index)\n",
    "print(cv_model[best_index].score(X_test, y_test))\n",
    "\n",
    "y_pred = cv_model[best_index].predict(X_test)\n",
    "y_prob = cv_model[best_index].predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_model/230730_RFC_larger_database_ICBHI_ref_max_cv_fold_10_train_with_9_with_scaler.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(cv_model[best_index], 'saved_model/230730_RFC_larger_database_ICBHI_ref_max_cv_fold_10_train_with_9_with_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with k-fold, without scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test with fold: 1 : 0.6521699927867276\n",
      "test with fold: 2 : 0.6149615292137534\n",
      "test with fold: 3 : 0.6173960086559269\n",
      "test with fold: 4 : 0.7603991344073094\n",
      "test with fold: 5 : 0.606275547006492\n",
      "test with fold: 6 : 0.7246333253185862\n",
      "test with fold: 7 : 0.718321711949988\n",
      "test with fold: 8 : 0.7499398893964896\n",
      "test with fold: 9 : 0.7640358259196922\n",
      "test with fold: 10 : 0.3539913440730945\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=False)\n",
    "\n",
    "cv_model = []\n",
    "cv_score = []\n",
    "\n",
    "fold_number = 0\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "    \n",
    "    fold_number += 1\n",
    "\n",
    "    clf = RandomForestClassifier(random_state=0)\n",
    "    \n",
    "    train_x, val_x = X[train_index], X[val_index]\n",
    "    train_y, val_y = y[train_index], y[val_index]\n",
    "    \n",
    "    clf.fit(train_x, train_y)\n",
    "    score = clf.score(val_x, val_y)\n",
    "    \n",
    "    cv_model.append(clf)\n",
    "    cv_score.append(score)\n",
    "    \n",
    "    print('test with fold:', fold_number, ':', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0.8452173913043478\n",
      "acc: 0.8452173913043478\n",
      "f1: 0.8509455354273302\n",
      "roc_auc: 0.960877767175696\n",
      "[[0.85563237 0.00569337 0.13867426]\n",
      " [0.04721754 0.82799325 0.12478921]\n",
      " [0.04727646 0.12332991 0.82939363]]\n"
     ]
    }
   ],
   "source": [
    "best_index = np.argmax(cv_score)\n",
    "print(best_index)\n",
    "print(cv_model[best_index].score(X_test, y_test))\n",
    "\n",
    "y_pred = cv_model[best_index].predict(X_test)\n",
    "y_prob = cv_model[best_index].predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_model/230730_RFC_larger_database_ICBHI_ref_max_cv_fold_10_train_with_9_without_scaler_fix_sample_rate_error.joblib']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(cv_model[best_index], 'saved_model/230730_RFC_larger_database_ICBHI_ref_max_cv_fold_10_train_with_9_without_scaler_fix_sample_rate_error.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with k-fold, shuffle, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test with fold: 1 : 0.8239360423178649\n",
      "test with fold: 2 : 0.8215616734792017\n",
      "test with fold: 3 : 0.8230944938687185\n",
      "test with fold: 4 : 0.8235152680932917\n",
      "test with fold: 5 : 0.8211108439528733\n",
      "test with fold: 6 : 0.825649194517913\n",
      "test with fold: 7 : 0.817534263044001\n",
      "test with fold: 8 : 0.8235753786968021\n",
      "test with fold: 9 : 0.8233048809810051\n",
      "test with fold: 10 : 0.8202993508054821\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "cv_model = []\n",
    "cv_score = []\n",
    "\n",
    "fold_number = 0\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "    \n",
    "    fold_number += 1\n",
    "    \n",
    "    clf = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=0))  \n",
    "#     clf = RandomForestClassifier(random_state=0)\n",
    "    \n",
    "    train_x, val_x = X[train_index], X[val_index]\n",
    "    train_y, val_y = y[train_index], y[val_index]\n",
    "    \n",
    "    clf.fit(train_x, train_y)\n",
    "    score = clf.score(val_x, val_y)\n",
    "    \n",
    "    cv_model.append(clf)\n",
    "    cv_score.append(score)\n",
    "    \n",
    "    print('test with fold:', fold_number, ':', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "0.8370186335403726\n",
      "acc: 0.8370186335403726\n",
      "f1: 0.842056913078042\n",
      "roc_auc: 0.952309017929985\n",
      "[[0.85278569 0.00244002 0.1447743 ]\n",
      " [0.08768971 0.82630691 0.08600337]\n",
      " [0.0647482  0.1315519  0.8036999 ]]\n"
     ]
    }
   ],
   "source": [
    "best_index = np.argmax(cv_score)\n",
    "print(best_index)\n",
    "print(cv_model[best_index].score(X_test, y_test))\n",
    "\n",
    "y_pred = cv_model[best_index].predict(X_test)\n",
    "y_prob = cv_model[best_index].predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_model/230730_RFC_larger_database_ICBHI_ref_max_cv_fold_10_train_with_9_with_shuffle_without_scaler.joblib']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(cv_model[best_index], 'saved_model/230730_RFC_larger_database_ICBHI_ref_max_cv_fold_10_train_with_9_with_shuffle_without_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without k-fold, scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8372670807453416\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(random_state=0)\n",
    "clf.fit(X, y)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8372670807453416\n",
      "f1: 0.8421785447225325\n",
      "roc_auc: 0.9530288238899\n",
      "[[0.85359902 0.00325336 0.14314762]\n",
      " [0.08768971 0.82124789 0.09106239]\n",
      " [0.06988695 0.12435766 0.8057554 ]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_model/230730_RFC_larger_database_ICBHI_ref_max_no_scaler.joblib']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, 'saved_model/230730_RFC_larger_database_ICBHI_ref_max_no_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without k-fold, with scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8357763975155279\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), RandomForestClassifier(random_state=0))\n",
    "clf.fit(X, y)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8357763975155279\n",
      "f1: 0.8409018850854336\n",
      "roc_auc: 0.9533355171262423\n",
      "[[0.84953233 0.00325336 0.14721431]\n",
      " [0.08600337 0.82630691 0.08768971]\n",
      " [0.0688592  0.12435766 0.80678314]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_model/230730_RFC_larger_database_ICBHI_ref_max_with_scaler.joblib']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, 'saved_model/230730_RFC_larger_database_ICBHI_ref_max_with_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearset Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with k-fold, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test with fold: 1 : 0.7484671796104833\n",
      "test with fold: 2 : 0.7974873767732628\n",
      "test with fold: 3 : 0.7285104592450108\n",
      "test with fold: 4 : 0.7365652801154123\n",
      "test with fold: 5 : 0.7112887713392643\n",
      "test with fold: 6 : 0.7180512142341909\n",
      "test with fold: 7 : 0.7118297667708584\n",
      "test with fold: 8 : 0.729321952392402\n",
      "test with fold: 9 : 0.7372565520557827\n",
      "test with fold: 10 : 0.3754207742245732\n",
      "1\n",
      "0.7642236024844721\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=False)\n",
    "cv_model = []\n",
    "cv_score = []\n",
    "\n",
    "fold_number = 0\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "    \n",
    "    fold_number += 1\n",
    "    \n",
    "    clf = make_pipeline(StandardScaler(), KNeighborsClassifier(3))      \n",
    "    \n",
    "    train_x, val_x = X[train_index], X[val_index]\n",
    "    train_y, val_y = y[train_index], y[val_index]\n",
    "    \n",
    "    clf.fit(train_x, train_y)\n",
    "    score = clf.score(val_x, val_y)\n",
    "    \n",
    "    cv_model.append(clf)\n",
    "    cv_score.append(score)\n",
    "    \n",
    "    print('test with fold:', fold_number, ':', score)\n",
    "    \n",
    "best_index = np.argmax(cv_score)\n",
    "print(best_index)\n",
    "print(cv_model[best_index].score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.7642236024844721\n",
      "acc: 0.7642236024844721\n",
      "f1: 0.73753981842823\n",
      "roc_auc: 0.7864435686152825\n",
      "[[0.97763318 0.00732005 0.01504677]\n",
      " [0.3693086  0.53625632 0.09443508]\n",
      " [0.42857143 0.20760534 0.36382323]]\n"
     ]
    }
   ],
   "source": [
    "best_index = np.argmax(cv_score)\n",
    "print(best_index)\n",
    "print(cv_model[best_index].score(X_test, y_test))\n",
    "\n",
    "y_pred = cv_model[best_index].predict(X_test)\n",
    "y_prob = cv_model[best_index].predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_model/230730_KNN_larger_database_ICBHI_ref_max_cv_fold_10_train_with_9_with_scaler.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(cv_model[best_index], 'saved_model/230730_KNN_larger_database_ICBHI_ref_max_cv_fold_10_train_with_9_with_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with k-fold, without scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test with fold: 1 : 0.6234972349122385\n",
      "test with fold: 2 : 0.5769115171916326\n",
      "test with fold: 3 : 0.5654003366193796\n",
      "test with fold: 4 : 0.6816542438086078\n",
      "test with fold: 5 : 0.4851827362346718\n",
      "test with fold: 6 : 0.5840947343111325\n",
      "test with fold: 7 : 0.5925703294061072\n",
      "test with fold: 8 : 0.6820149074296706\n",
      "test with fold: 9 : 0.6846597739841308\n",
      "test with fold: 10 : 0.29868958884347196\n",
      "8\n",
      "0.8233540372670808\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=False)\n",
    "cv_model = []\n",
    "cv_score = []\n",
    "\n",
    "fold_number = 0\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "    \n",
    "    fold_number += 1\n",
    "    \n",
    "    clf = KNeighborsClassifier(3)     \n",
    "    \n",
    "    train_x, val_x = X[train_index], X[val_index]\n",
    "    train_y, val_y = y[train_index], y[val_index]\n",
    "    \n",
    "    clf.fit(train_x, train_y)\n",
    "    score = clf.score(val_x, val_y)\n",
    "    \n",
    "    cv_model.append(clf)\n",
    "    cv_score.append(score)\n",
    "    \n",
    "    print('test with fold:', fold_number, ':', score)\n",
    "    \n",
    "best_index = np.argmax(cv_score)\n",
    "print(best_index)\n",
    "print(cv_model[best_index].score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "0.8233540372670808\n",
      "acc: 0.8233540372670808\n",
      "f1: 0.8269936614885541\n",
      "roc_auc: 0.9202266846916052\n",
      "[[0.85237902 0.01220008 0.1354209 ]\n",
      " [0.10455312 0.84317032 0.05227656]\n",
      " [0.10894142 0.15313464 0.73792395]]\n"
     ]
    }
   ],
   "source": [
    "best_index = np.argmax(cv_score)\n",
    "print(best_index)\n",
    "print(cv_model[best_index].score(X_test, y_test))\n",
    "\n",
    "y_pred = cv_model[best_index].predict(X_test)\n",
    "y_prob = cv_model[best_index].predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_model/230730_KNN_larger_database_ICBHI_ref_max_cv_fold_10_train_with_9_without_scaler_fix_sample_rate_error.joblib']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(cv_model[best_index], 'saved_model/230730_KNN_larger_database_ICBHI_ref_max_cv_fold_10_train_with_9_without_scaler_fix_sample_rate_error.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without k-fold, with scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7607453416149068\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), KNeighborsClassifier(3))    \n",
    "clf.fit(X, y)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7607453416149068\n",
      "f1: 0.7331757895074731\n",
      "roc_auc: 0.7802691847024038\n",
      "[[0.97681985 0.00772672 0.01545344]\n",
      " [0.37099494 0.52613828 0.10286678]\n",
      " [0.44295992 0.19938335 0.35765673]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_model/230730_KNN_larger_database_ICBHI_ref_max_with_scaler.joblib']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, 'saved_model/230730_KNN_larger_database_ICBHI_ref_max_with_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without k-fold, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8260869565217391\n"
     ]
    }
   ],
   "source": [
    "clf = KNeighborsClassifier(3)    \n",
    "clf.fit(X, y)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.8260869565217391\n",
      "f1: 0.8294645570249439\n",
      "roc_auc: 0.9202688934836653\n",
      "[[0.85725905 0.00732005 0.1354209 ]\n",
      " [0.10455312 0.84317032 0.05227656]\n",
      " [0.11408016 0.14902364 0.7368962 ]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_model/230730_KNN_larger_database_ICBHI_ref_max_without_scaler.joblib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, 'saved_model/230730_KNN_larger_database_ICBHI_ref_max_without_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Process Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with k-fold, scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 668. GiB for an array with shape (299448, 299448) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-f8eebb013a78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/gaussian_process/_gpc.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    655\u001b[0m                                  % self.multi_class)\n\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0;34m\"not %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_binarizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 self.label_binarizer_.classes_[i]])\n\u001b[0;32m--> 245\u001b[0;31m             for i, column in enumerate(columns))\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1048\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    864\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 784\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    785\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/multiclass.py\u001b[0m in \u001b[0;36m_fit_binary\u001b[0;34m(estimator, X, y, classes)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/gaussian_process/_gpc.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    212\u001b[0m             optima = [self._constrained_optimization(obj_func,\n\u001b[1;32m    213\u001b[0m                                                      \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m                                                      self.kernel_.bounds)]\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;31m# Additional runs are performed from log-uniform chosen initial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/gaussian_process/_gpc.py\u001b[0m in \u001b[0;36m_constrained_optimization\u001b[0;34m(self, obj_func, initial_theta, bounds)\u001b[0m\n\u001b[1;32m    443\u001b[0m             opt_res = scipy.optimize.minimize(\n\u001b[1;32m    444\u001b[0m                 \u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"L-BFGS-B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                 bounds=bounds)\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0m_check_optimize_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lbfgs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0mtheta_opt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m--> 610\u001b[0;31m                                 callback=callback, **options)\n\u001b[0m\u001b[1;32m    611\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m         return _minimize_tnc(fun, x0, args, jac, bounds, callback=callback,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, **unknown_options)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36mfunc_and_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m             \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36mfunction_wrapper\u001b[0;34m(*wrapper_args)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwrapper_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mncalls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapper_args\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mncalls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/gaussian_process/_gpc.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(theta, eval_gradient)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                     lml, grad = self.log_marginal_likelihood(\n\u001b[0;32m--> 205\u001b[0;31m                         theta, eval_gradient=True, clone_kernel=False)\n\u001b[0m\u001b[1;32m    206\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/gaussian_process/_gpc.py\u001b[0m in \u001b[0;36mlog_marginal_likelihood\u001b[0;34m(self, theta, eval_gradient, clone_kernel)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/gaussian_process/kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[1;32m    850\u001b[0m         \"\"\"\n\u001b[1;32m    851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m             \u001b[0mK1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK1_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m             \u001b[0mK2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK2_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             return K1 * K2, np.dstack((K1_gradient * K2[:, :, np.newaxis],\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/gaussian_process/kernels.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, X, Y, eval_gradient)\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m         K = np.full((_num_samples(X), _num_samples(Y)), self.constant_value,\n\u001b[0;32m-> 1161\u001b[0;31m                     dtype=np.array(self.constant_value).dtype)\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meval_gradient\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_constant_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36mfull\u001b[0;34m(shape, fill_value, dtype, order)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m     \u001b[0mmultiarray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unsafe'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 668. GiB for an array with shape (299448, 299448) and data type float64"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=False)\n",
    "cv_model = []\n",
    "cv_score = []\n",
    "\n",
    "fold_number = 0\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "    \n",
    "    fold_number += 1\n",
    "    \n",
    "    clf = make_pipeline(StandardScaler(), GaussianProcessClassifier(1.0 * RBF(1.0), random_state=0)) \n",
    "    \n",
    "    train_x, val_x = X[train_index], X[val_index]\n",
    "    train_y, val_y = y[train_index], y[val_index]\n",
    "    \n",
    "    clf.fit(train_x, train_y)\n",
    "    score = clf.score(val_x, val_y)\n",
    "    \n",
    "    cv_model.append(clf)\n",
    "    cv_score.append(score)\n",
    "    \n",
    "    print('test with fold:', fold_number, ':', score)\n",
    "    \n",
    "best_index = np.argmax(cv_score)\n",
    "print(best_index)\n",
    "print(cv_model[best_index].score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index = np.argmax(cv_score)\n",
    "print(best_index)\n",
    "print(cv_model[best_index].score(X_test, y_test))\n",
    "\n",
    "y_pred = cv_model[best_index].predict(X_test)\n",
    "y_prob = cv_model[best_index].predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(cv_model[best_index], 'saved_model/230730_GPC_larger_database_ICBHI_ref_max_cv_fold_10_train_with_9_with_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without k-fold, with scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = make_pipeline(StandardScaler(), GaussianProcessClassifier(random_state=0)) \n",
    "clf.fit(X, y)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(cv_model[best_index], 'saved_model/230730_GPC_larger_database_ICBHI_ref_max_with_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without k-fold, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianProcessClassifier(random_state=0)\n",
    "clf.fit(X, y)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(cv_model[best_index], 'saved_model/230730_GPC_larger_database_ICBHI_ref_max_without_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Layer Perceptron "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with k-fold,scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test with fold: 1 : 0.7944517912959846\n",
      "test with fold: 2 : 0.8429911036306804\n",
      "test with fold: 3 : 0.77028732868478\n",
      "test with fold: 4 : 0.7786727578744891\n",
      "test with fold: 5 : 0.7797246934359221\n",
      "test with fold: 6 : 0.7704075498918009\n",
      "test with fold: 7 : 0.7631041115652801\n",
      "test with fold: 8 : 0.7354832892522241\n",
      "test with fold: 9 : 0.7980283722048569\n",
      "test with fold: 10 : 0.387953835056504\n",
      "1\n",
      "0.7741614906832298\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=False)\n",
    "cv_model = []\n",
    "cv_score = []\n",
    "\n",
    "fold_number = 0\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "    \n",
    "    fold_number += 1\n",
    "    \n",
    "    clf = make_pipeline(StandardScaler(), MLPClassifier(early_stopping=True, shuffle=False, random_state=0, verbose=0)) \n",
    "    \n",
    "    train_x, val_x = X[train_index], X[val_index]\n",
    "    train_y, val_y = y[train_index], y[val_index]\n",
    "    \n",
    "    clf.fit(train_x, train_y)\n",
    "    score = clf.score(val_x, val_y)\n",
    "    \n",
    "    cv_model.append(clf)\n",
    "    cv_score.append(score)\n",
    "    \n",
    "    print('test with fold:', fold_number, ':', score)\n",
    "    \n",
    "best_index = np.argmax(cv_score)\n",
    "print(best_index)\n",
    "print(cv_model[best_index].score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.7741614906832298\n",
      "acc: 0.7741614906832298\n",
      "f1: 0.7561563232858209\n",
      "roc_auc: 0.8383847354529177\n",
      "[[0.96990647 0.01098007 0.01911346]\n",
      " [0.19561551 0.46711636 0.33726813]\n",
      " [0.37101747 0.16238438 0.46659815]]\n"
     ]
    }
   ],
   "source": [
    "best_index = np.argmax(cv_score)\n",
    "print(best_index)\n",
    "print(cv_model[best_index].score(X_test, y_test))\n",
    "\n",
    "y_pred = cv_model[best_index].predict(X_test)\n",
    "y_prob = cv_model[best_index].predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_model/230730_MLP_larger_database_ICBHI_ref_max_cv_fold_10_train_with_9_with_scaler.joblib']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(cv_model[best_index], 'saved_model/230730_MLP_larger_database_ICBHI_ref_max_cv_fold_10_train_with_9_with_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with k-fold without scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test with fold: 1 : 0.6268634287088243\n",
      "test with fold: 2 : 0.6179370040875211\n",
      "test with fold: 3 : 0.588092089444578\n",
      "test with fold: 4 : 0.7578143784563597\n",
      "test with fold: 5 : 0.5489300312575138\n",
      "test with fold: 6 : 0.6513885549410916\n",
      "test with fold: 7 : 0.6453474392882904\n",
      "test with fold: 8 : 0.7183517672517432\n",
      "test with fold: 9 : 0.7030235633565761\n",
      "test with fold: 10 : 0.3172637653282039\n",
      "3\n",
      "0.8683229813664596\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=False)\n",
    "cv_model = []\n",
    "cv_score = []\n",
    "\n",
    "fold_number = 0\n",
    "\n",
    "for train_index, val_index in kfold.split(X):\n",
    "    \n",
    "    fold_number += 1\n",
    "    \n",
    "    clf = MLPClassifier(early_stopping=True, shuffle=False, random_state=0, verbose=0)\n",
    "    \n",
    "    train_x, val_x = X[train_index], X[val_index]\n",
    "    train_y, val_y = y[train_index], y[val_index]\n",
    "    \n",
    "    clf.fit(train_x, train_y)\n",
    "    score = clf.score(val_x, val_y)\n",
    "    \n",
    "    cv_model.append(clf)\n",
    "    cv_score.append(score)\n",
    "    \n",
    "    print('test with fold:', fold_number, ':', score)\n",
    "    \n",
    "best_index = np.argmax(cv_score)\n",
    "print(best_index)\n",
    "print(cv_model[best_index].score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "0.8683229813664596\n",
      "acc: 0.8683229813664596\n",
      "f1: 0.8704820265679196\n",
      "roc_auc: 0.9490209603170922\n",
      "[[0.91541277 0.01952013 0.0650671 ]\n",
      " [0.03541315 0.70320405 0.2613828 ]\n",
      " [0.06372045 0.08633094 0.84994861]]\n"
     ]
    }
   ],
   "source": [
    "best_index = np.argmax(cv_score)\n",
    "print(best_index)\n",
    "print(cv_model[best_index].score(X_test, y_test))\n",
    "\n",
    "y_pred = cv_model[best_index].predict(X_test)\n",
    "y_prob = cv_model[best_index].predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_model/230730_MLP_larger_database_ICBHI_ref_max_cv_fold_10_train_with_9_without_scaler_fix_sample_rate_error.joblib']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(cv_model[best_index], 'saved_model/230730_MLP_larger_database_ICBHI_ref_max_cv_fold_10_train_with_9_without_scaler_fix_sample_rate_error.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without k-fold, scaler, fix sample rate error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7803726708074534\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(early_stopping=True, shuffle=False, random_state=0, verbose=0)\n",
    "clf.fit(X, y)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7803726708074534\n",
      "f1: 0.790658200471098\n",
      "roc_auc: 0.9441566107889893\n",
      "[[0.78527857 0.00610004 0.20862139]\n",
      " [0.07588533 0.59865093 0.32546374]\n",
      " [0.05138746 0.06988695 0.87872559]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without k-fold, scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7669565217391304\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(early_stopping=True, shuffle=False, random_state=0, verbose=0)\n",
    "clf.fit(X, y)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.7669565217391304\n",
      "f1: 0.7684368659991588\n",
      "roc_auc: 0.9213942807368825\n",
      "[[0.83326556 0.         0.16673444]\n",
      " [0.12310287 0.37605396 0.50084317]\n",
      " [0.08633094 0.07605344 0.83761562]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_model/230730_MLP_larger_database_ICBHI_ref_max_without_scaler.joblib']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, 'saved_model/230730_MLP_larger_database_ICBHI_ref_max_without_scaler.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without k-fold, with scaler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6581366459627329\n"
     ]
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), MLPClassifier(early_stopping=True, shuffle=False, random_state=0, verbose=0))\n",
    "clf.fit(X, y)\n",
    "score = clf.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6581366459627329\n",
      "f1: 0.6897669924392718\n",
      "roc_auc: 0.8452456502867675\n",
      "[[0.71492477 0.27206181 0.01301342]\n",
      " [0.02866779 0.72175379 0.24957841]\n",
      " [0.15724563 0.36690647 0.47584789]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "y_prob = clf.predict_proba(X_test)\n",
    "\n",
    "acc =accuracy_score(y_test,y_pred)\n",
    "f1=f1_score(y_test,y_pred,average='weighted')\n",
    "\n",
    "y_true = utils.to_categorical(y_test)\n",
    "\n",
    "roc_auc = roc_auc_score(y_true, y_prob, multi_class='ovr')\n",
    "cm = confusion_matrix(y_test, y_pred, normalize='true')\n",
    "\n",
    "print('acc:', acc)\n",
    "print('f1:', f1)\n",
    "print('roc_auc:', roc_auc)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['saved_model/230730_MLP_larger_database_ICBHI_ref_max_with_scaler.joblib']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, 'saved_model/230730_MLP_larger_database_ICBHI_ref_max_with_scaler.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
